{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data as Data\n",
    "import numpy as np\n",
    "import optuna\n",
    "from EduKTM import DKT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_QUESTIONS = 9\n",
    "\n",
    "def get_train_val_loaders(batch_size, val_split=0.2, shuffle=True, data_percentage=1.0):\n",
    "    # Load the entire dataset\n",
    "    data = torch.FloatTensor(np.load('train_data.npy'))\n",
    "    \n",
    "    # Apply data_percentage to reduce dataset size if needed\n",
    "    if data_percentage < 1.0:\n",
    "        total_samples = len(data)\n",
    "        samples_to_keep = int(total_samples * data_percentage)\n",
    "        if shuffle:\n",
    "            indices = torch.randperm(total_samples)[:samples_to_keep]\n",
    "            data = data[indices]\n",
    "        else:\n",
    "            data = data[:samples_to_keep]\n",
    "    \n",
    "    # Get dataset size and calculate split\n",
    "    dataset_size = len(data)\n",
    "    val_size = int(dataset_size * val_split)\n",
    "    train_size = dataset_size - val_size\n",
    "    \n",
    "    # Split the dataset\n",
    "    if shuffle:\n",
    "        indices = torch.randperm(dataset_size)\n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:]\n",
    "        train_data = data[train_indices]\n",
    "        val_data = data[val_indices]\n",
    "    else:\n",
    "        train_data = data[:train_size]\n",
    "        val_data = data[train_size:]\n",
    "    \n",
    "    # Create data loaders\n",
    "    train_loader = Data.DataLoader(train_data, batch_size=batch_size, shuffle=shuffle)\n",
    "    val_loader = Data.DataLoader(val_data, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f\"Training samples: {train_size}, Validation samples: {val_size}\")\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "def get_test_data_loader(batch_size, shuffle=False, data_percentage=1.0):\n",
    "    data = torch.FloatTensor(np.load('test_data.npy'))\n",
    "    # Select only a percentage of the data\n",
    "    if data_percentage < 1.0:\n",
    "        total_samples = len(data)\n",
    "        samples_to_keep = int(total_samples * data_percentage)\n",
    "        if shuffle:\n",
    "            indices = torch.randperm(total_samples)[:samples_to_keep]\n",
    "            data = data[indices]\n",
    "        else:\n",
    "            data = data[:samples_to_keep]\n",
    "    \n",
    "    data_loader = Data.DataLoader(data, batch_size=batch_size, shuffle=shuffle)\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def importance_objective(trial):\n",
    "    # Define hyperparameters to optimize\n",
    "    hidden_size = trial.suggest_categorical('hidden_size', [5, 10, 20, 50, 100])\n",
    "    num_layer = trial.suggest_int('num_layers', 1, 3)\n",
    "    batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])\n",
    "    lr = trial.suggest_float('learning_rate', 1e-4, 1e-1, log=True)\n",
    "    \n",
    "    train_subset_loader, val_subset_loader = get_train_val_loaders(batch_size, data_percentage=0.3)\n",
    "    # test_subset_loader = get_test_data_loader( './test_data.npy', batch_size, False, 0.1)\n",
    "    \n",
    "    # Initialize and train model\n",
    "    dkt_model = DKT(NUM_QUESTIONS, hidden_size, num_layer)\n",
    "    dkt_model.train(train_subset_loader, val_subset_loader, epoch=10, lr=lr)\n",
    "    \n",
    "    # Return the AUC score to be maximized\n",
    "    return dkt_model.eval(val_subset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_study = optuna.create_study(study_name=\"dkt_importances\", storage=\"sqlite:///../studies.db\", load_if_exists=True, direction='maximize')\n",
    "importance_study.optimize(importance_objective, n_trials=100, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Best parameters: {importance_study.best_params}\")\n",
    "print(f\"Best AUC: {importance_study.best_value}\")\n",
    "\n",
    "optuna.visualization.plot_optimization_history(importance_study)\n",
    "optuna.visualization.plot_param_importances(importance_study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = importance_study.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learning_rate_objective(trial):\n",
    "    # Use the best parameters from importance_study but optimize learning rate\n",
    "    hidden_size = best_params['hidden_size']\n",
    "    num_layers = best_params['num_layers']\n",
    "    batch_size = best_params['batch_size']\n",
    "    \n",
    "    # Define learning rate range to optimize\n",
    "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # Use smaller subset for faster optimization\n",
    "    train_subset_loader, val_subset_loader = get_train_val_loaders(batch_size, data_percentage=0.4)\n",
    "    \n",
    "    # Initialize model with best parameters from previous study\n",
    "    dkt_model = DKT(NUM_QUESTIONS, hidden_size, num_layers)\n",
    "    dkt_model.train(train_subset_loader, val_subset_loader, epoch=15, lr=lr)\n",
    "    \n",
    "    # Return validation AUC to be maximized\n",
    "    return dkt_model.eval(val_subset_loader)\n",
    "\n",
    "# Create a new study focused on learning rate optimization\n",
    "lr_study = optuna.create_study(study_name=\"dkt_learning_rate\", storage=\"sqlite:///../studies.db\", load_if_exists=True, direction='maximize')\n",
    "lr_study.optimize(learning_rate_objective, n_trials=30, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(f\"Best learning rate: {lr_study.best_params['learning_rate']}\")\n",
    "print(f\"Best AUC with optimized learning rate: {lr_study.best_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loader, val_loader = get_train_val_loaders(batch_size=64)\n",
    "\n",
    "dkt = DKT(NUM_QUESTIONS, hidden_size=100, num_layers=3)\n",
    "dkt.train(train_loader, val_loader, epoch=50)\n",
    "dkt.save(\"dkt.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = get_test_data_loader(batch_size=best_params['batch_size'])\n",
    "\n",
    "dkt.load(\"dkt.params\")\n",
    "auc = dkt.eval(test_loader)\n",
    "print(\"auc: %.6f\" % auc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
