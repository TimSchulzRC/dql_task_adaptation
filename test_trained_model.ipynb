{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.contrib import itertools\n",
    "from simulation_util import calc_frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EduKTM import DKT, AKT\n",
    "import torch\n",
    "import torch.utils.data as Data\n",
    "from dkt.loader import transform_sim_data, parse_all_seq, encode_onehot\n",
    "import tqdm\n",
    "from akt.load_data import DATA, PID_DATA\n",
    "\n",
    "NUM_QUESTIONS = 123\n",
    "HIDDEN_SIZE = 10\n",
    "NUM_LAYERS = 1\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "dkt = DKT(NUM_QUESTIONS, HIDDEN_SIZE, NUM_LAYERS)\n",
    "dkt.load(\"./dkt/assistment-2009-2010-skilldkt.params\")\n",
    "dkt_model = dkt.dkt_model\n",
    "dkt_model.eval()\n",
    "\n",
    "\n",
    "def get_competency_with_dkt_model(simulation_log, dql_model):\n",
    "    transformed_data = transform_sim_data(simulation_log, dql_model).dropna(subset=['skill_id'])\n",
    "    data = parse_all_seq(transformed_data)\n",
    "    encoded = encode_onehot(data, NUM_QUESTIONS)\n",
    "    data_loader = Data.DataLoader(encoded, shuffle=True)\n",
    "    \n",
    "    for batch in data_loader:\n",
    "        integrated_pred = dkt_model(batch.to(torch.float32))\n",
    "        task_count = integrated_pred.shape[1]\n",
    "    return {\n",
    "        category: [\n",
    "            calc_frequency(\n",
    "                integrated_pred[0][task_count-1][(category_i * len(dql_model[category])) + i].item()\n",
    "            ) for i in range(len(dql_model[category]))\n",
    "        ] for category_i, category in enumerate(dql_model)\n",
    "    }\n",
    "\n",
    "n_question = 123 # from prepare_dataset\n",
    "n_pid = 0 # from prepare_dataset, 0 if not used\n",
    "seqlen = 200\n",
    "\n",
    "model_type = 'pid'\n",
    "batch_size = 64\n",
    "n_blocks = 1\n",
    "d_model = 256\n",
    "dropout = 0.05\n",
    "kq_same = 1\n",
    "l2 = 1e-5\n",
    "maxgradnorm = -1\n",
    "\n",
    "akt = AKT(n_question, n_pid, n_blocks, d_model, dropout, kq_same, l2, batch_size, maxgradnorm)\n",
    "akt.load(\"./akt/akt.params\")\n",
    "akt_model = akt.akt_net\n",
    "akt_model.eval()\n",
    "\n",
    "def get_competency_with_akt_model(simulation_log, dql_model):\n",
    "    data = transform_sim_data(simulation_log, dql_model)\n",
    "    raw_skill = data.skill_id.unique().tolist()\n",
    "    raw_problem = data.problem_id.unique().tolist()\n",
    "    num_skill = len(raw_skill)\n",
    "    n_problem = len(raw_problem)\n",
    "\n",
    "    # question id from 1 to #num_skill\n",
    "    skills = { p: i+1 for i, p in enumerate(raw_skill) }\n",
    "    problems = { p: i+1 for i, p in enumerate(raw_problem) }\n",
    "    \n",
    "    def parse_all_seq(students):\n",
    "        all_sequences = []\n",
    "        for student_id in tqdm.tqdm(students, 'parse student sequence:\\t'):\n",
    "            student_sequence = parse_student_seq(data[data.user_id == student_id])\n",
    "            all_sequences.extend([student_sequence])\n",
    "        return all_sequences\n",
    "\n",
    "\n",
    "    def parse_student_seq(student):\n",
    "        seq = student.sort_values('order_id')\n",
    "        s = [skills[q] for q in seq.skill_id.tolist()]\n",
    "        p = [problems[q] for q in seq.problem_id.tolist()]\n",
    "        a = seq.correct.tolist()\n",
    "        return s, p, a\n",
    "\n",
    "    sequences = parse_all_seq(data.user_id.unique())\n",
    "    \n",
    "    \n",
    "    for batch in data_loader:\n",
    "        integrated_pred = akt_model(batch.to(torch.float32))\n",
    "        task_count = integrated_pred.shape[1]\n",
    "    return {\n",
    "        category: [\n",
    "            calc_frequency(\n",
    "                integrated_pred[0][task_count-1][(category_i * len(dql_model[category])) + i].item()\n",
    "            ) for i in range(len(dql_model[category]))\n",
    "        ] for category_i, category in enumerate(dql_model)\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from simulation_util import add_delta_to_competency, calc_task_complexities, calculate_delta, create_learner_population, create_learner_scaffolded_competence_bonuses, create_optimal_task, create_random_task\n",
    "from simulation_const import DQL_MODEL\n",
    "\n",
    "MAX_TASK_COUNT = 100\n",
    "\n",
    "def simulate_task_adaptation(learner_count: int, mean_competency: float, bonus_distribution: tuple[float, float, float, float], type: str = \"optimal\"):\n",
    "    \"\"\"\n",
    "    Simulate task adaptation with different strategies for task creation.\n",
    "    \n",
    "    Args:\n",
    "        learner_count: Number of learners to simulate\n",
    "        mean_competency: Mean competency level for the learner population\n",
    "        bonus_distribution: Tuple of 4 values for scaffolding bonus distribution\n",
    "        task_type: Strategy for task creation (\"optimal\", \"random\", \"dkt\", or \"akt\")\n",
    "    \"\"\"\n",
    "    if type not in [\"optimal\", \"random\", \"dkt\", \"akt\"]:\n",
    "        raise ValueError(\"task_type must be one of: optimal, random, dkt, akt\")\n",
    "\n",
    "    learner_population = create_learner_population(\n",
    "        learner_count, MAX_TASK_COUNT, DQL_MODEL, mean_competency, bonus_distribution)\n",
    "\n",
    "    simulationLog = []\n",
    "\n",
    "    for i, j in itertools.product(range(learner_count), range(MAX_TASK_COUNT)):\n",
    "\n",
    "        learner_competency = learner_population[\"learner_competencies\"][i]\n",
    "        scaffolding_bonus = learner_population[\"scaffolding_competence_bonus_per_step_and_learner\"][j][i]\n",
    "        \n",
    "        # Create task based on selected strategy\n",
    "        if type == \"optimal\":\n",
    "            task = create_optimal_task(DQL_MODEL, learner_competency, scaffolding_bonus)\n",
    "        elif type == \"random\":\n",
    "            task = create_random_task(DQL_MODEL)\n",
    "        elif type == \"dkt\":\n",
    "            # First task is random since there's no previous data\n",
    "            if i == 0:\n",
    "                task = create_random_task(DQL_MODEL)\n",
    "            else:\n",
    "                competency = get_competency_with_dkt_model([simulationLog[i-1]], DQL_MODEL)\n",
    "                bonus = create_learner_scaffolded_competence_bonuses(DQL_MODEL, bonus_distribution)\n",
    "                task = {\n",
    "                    category: [\n",
    "                        competency[category][i] + bonus[category][i]\n",
    "                        for i in range(len(competency[category]))\n",
    "                    ] for category in competency\n",
    "                }\n",
    "        elif type == \"akt\":\n",
    "            # First task is random since there's no previous data\n",
    "            if i == 0:\n",
    "                task = create_random_task(DQL_MODEL)\n",
    "            else:\n",
    "                competency = get_competency_with_akt_model([simulationLog[i-1]], DQL_MODEL)\n",
    "                bonus = create_learner_scaffolded_competence_bonuses(DQL_MODEL, bonus_distribution)\n",
    "                task = {\n",
    "                    category: [\n",
    "                        competency[category][i] + bonus[category][i]\n",
    "                        for i in range(len(competency[category]))\n",
    "                    ] for category in competency\n",
    "                }\n",
    "        else:\n",
    "            break\n",
    "\n",
    "        task_complexities = calc_task_complexities(task)\n",
    "        \n",
    "        delta = calculate_delta(\n",
    "            learner_competency, task_complexities, scaffolding_bonus)\n",
    "        \n",
    "        # if i == 1:\n",
    "        #     print(\"Old competency: \", learner_competency)\n",
    "        #     print(delta)\n",
    "        #     print(\"New competency: \", add_delta_to_competency(learner_competency, delta))\n",
    "        #     print(\"\\n\")\n",
    "\n",
    "        # update the learner competency in the global learner population\n",
    "        learner_population[\"learner_competencies\"][i] = add_delta_to_competency(\n",
    "            learner_competency, delta)\n",
    "\n",
    "        simulationLog.append({\n",
    "            \"tasks\": [],\n",
    "            \"competencies\": [],\n",
    "            \"scaffolding_bonuses\": [],\n",
    "            \"deltas\": []\n",
    "        })\n",
    "        simulationLog[i][\"tasks\"].append(task)\n",
    "        simulationLog[i][\"competencies\"].append(learner_competency)\n",
    "        simulationLog[i][\"scaffolding_bonuses\"].append(scaffolding_bonus)\n",
    "        simulationLog[i][\"deltas\"].append(delta)\n",
    "            \n",
    "    return simulationLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:01<00:00, 7577.86it/s]\n",
      "100%|██████████| 10000/10000 [00:01<00:00, 6959.24it/s]\n",
      "  1%|          | 100/10000 [00:00<00:06, 1645.57it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "AKTNet.forward() missing 2 required positional arguments: 'qa_data' and 'target'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m log_random = simulate_task_adaptation(\u001b[32m100\u001b[39m, \u001b[32m0.5\u001b[39m, (\u001b[32m0.1\u001b[39m, \u001b[32m0.002\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m0.2\u001b[39m), \u001b[38;5;28mtype\u001b[39m=\u001b[33m\"\u001b[39m\u001b[33mrandom\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# log_dkt = simulate_task_adaptation(100, 0.5, (0.1, 0.002, 0, 0.2), type=\"dkt\")\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m akt_log = \u001b[43msimulate_task_adaptation\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.002\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43makt\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36msimulate_task_adaptation\u001b[39m\u001b[34m(learner_count, mean_competency, bonus_distribution, type)\u001b[39m\n\u001b[32m     50\u001b[39m     task = create_random_task(DQL_MODEL)\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     competency = \u001b[43mget_competency_with_akt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43msimulationLog\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDQL_MODEL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m     bonus = create_learner_scaffolded_competence_bonuses(DQL_MODEL, bonus_distribution)\n\u001b[32m     54\u001b[39m     task = {\n\u001b[32m     55\u001b[39m         category: [\n\u001b[32m     56\u001b[39m             competency[category][i] + bonus[category][i]\n\u001b[32m     57\u001b[39m             \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(competency[category]))\n\u001b[32m     58\u001b[39m         ] \u001b[38;5;28;01mfor\u001b[39;00m category \u001b[38;5;129;01min\u001b[39;00m competency\n\u001b[32m     59\u001b[39m     }\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 59\u001b[39m, in \u001b[36mget_competency_with_akt_model\u001b[39m\u001b[34m(simulation_log, dql_model)\u001b[39m\n\u001b[32m     56\u001b[39m data_loader = Data.DataLoader(encoded, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[32m---> \u001b[39m\u001b[32m59\u001b[39m     integrated_pred = \u001b[43makt_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     60\u001b[39m     task_count = integrated_pred.shape[\u001b[32m1\u001b[39m]\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m     62\u001b[39m     category: [\n\u001b[32m     63\u001b[39m         calc_frequency(\n\u001b[32m   (...)\u001b[39m\u001b[32m     66\u001b[39m     ] \u001b[38;5;28;01mfor\u001b[39;00m category_i, category \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dql_model)\n\u001b[32m     67\u001b[39m }\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\dql_task_adaptation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\projects\\dql_task_adaptation\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[31mTypeError\u001b[39m: AKTNet.forward() missing 2 required positional arguments: 'qa_data' and 'target'"
     ]
    }
   ],
   "source": [
    "from simulation_plot import plot_mean_simulation_log, plot_simulation_log\n",
    "\n",
    "log_optimal = simulate_task_adaptation(100, 0.5, (0.1, 0.002, 0, 0.2), type=\"optimal\")\n",
    "log_random = simulate_task_adaptation(100, 0.5, (0.1, 0.002, 0, 0.2), type=\"random\")\n",
    "# log_dkt = simulate_task_adaptation(100, 0.5, (0.1, 0.002, 0, 0.2), type=\"dkt\")\n",
    "akt_log = simulate_task_adaptation(100, 0.5, (0.1, 0.002, 0, 0.2), type=\"akt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_simulation_log(log_optimal, 1)\n",
    "plot_simulation_log(log_random, 1)\n",
    "# plot_simulation_log(log_dkt, 1)\n",
    "plot_simulation_log(akt_log, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
