{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AKT Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn.init import xavier_uniform_, constant_\n",
    "import torch.nn.functional as F\n",
    "from enum import IntEnum\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Dim(IntEnum):\n",
    "    batch = 0\n",
    "    seq = 1\n",
    "    feature = 2\n",
    "\n",
    "\n",
    "class AKTNet(nn.Module):\n",
    "    def __init__(self, n_question, n_pid, d_model, n_blocks, kq_same, dropout, final_fc_dim=512, n_heads=8,\n",
    "                 d_ff=2048, l2=1e-5, separate_qa=False):\n",
    "        super(AKTNet, self).__init__()\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            d_model: dimension of attention block\n",
    "            final_fc_dim: dimension of final fully connected net before prediction\n",
    "            n_heads: number of heads in multi-headed attention\n",
    "            d_ff : dimension for fully connected net inside the basic block\n",
    "        \"\"\"\n",
    "        self.n_question = n_question\n",
    "        self.dropout = dropout\n",
    "        self.kq_same = kq_same\n",
    "        self.n_pid = n_pid\n",
    "        self.l2 = l2\n",
    "        self.separate_qa = separate_qa\n",
    "        embed_l = d_model\n",
    "        if self.n_pid > 0:\n",
    "            self.difficult_param = nn.Embedding(self.n_pid + 1, 1)\n",
    "            self.q_embed_diff = nn.Embedding(self.n_question + 1, embed_l)\n",
    "            self.qa_embed_diff = nn.Embedding(2 * self.n_question + 1, embed_l)\n",
    "        # n_question+1 ,d_model\n",
    "        self.q_embed = nn.Embedding(self.n_question + 1, embed_l)\n",
    "        if self.separate_qa:\n",
    "            self.qa_embed = nn.Embedding(2 * self.n_question + 1, embed_l)\n",
    "        else:\n",
    "            self.qa_embed = nn.Embedding(2, embed_l)\n",
    "        # Architecture Object. It contains stack of attention block\n",
    "        self.model = Architecture(n_blocks, d_model, d_model // n_heads, d_ff, n_heads, dropout, kq_same)\n",
    "\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(d_model + embed_l, final_fc_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(final_fc_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(self.dropout),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        for p in self.parameters():\n",
    "            if p.size(0) == self.n_pid + 1 and self.n_pid > 0:\n",
    "                constant_(p, 0.)\n",
    "\n",
    "    def forward(self, q_data, qa_data, target, pid_data=None):\n",
    "        # Batch First\n",
    "        q_embed_data = self.q_embed(q_data)\n",
    "        if self.separate_qa:\n",
    "            qa_embed_data = self.qa_embed(qa_data)\n",
    "        else:\n",
    "            qa_data = (qa_data - q_data) // self.n_question\n",
    "            qa_embed_data = self.qa_embed(qa_data) + q_embed_data\n",
    "\n",
    "        if self.n_pid > 0:\n",
    "            q_embed_diff_data = self.q_embed_diff(q_data)\n",
    "            pid_embed_data = self.difficult_param(pid_data)\n",
    "            q_embed_data = q_embed_data + pid_embed_data * q_embed_diff_data\n",
    "            qa_embed_diff_data = self.qa_embed_diff(qa_data)\n",
    "            if self.separate_qa:\n",
    "                qa_embed_data = qa_embed_data + pid_embed_data * qa_embed_diff_data\n",
    "            else:\n",
    "                qa_embed_data = qa_embed_data + pid_embed_data * (qa_embed_diff_data + q_embed_diff_data)\n",
    "            c_reg_loss = (pid_embed_data ** 2.).sum() * self.l2\n",
    "        else:\n",
    "            c_reg_loss = 0.\n",
    "\n",
    "        # BS.seqlen,d_model\n",
    "        # Pass to the decoder\n",
    "        # output shape BS,seqlen,d_model or d_model//2\n",
    "        d_output = self.model(q_embed_data, qa_embed_data)\n",
    "\n",
    "        concat_q = torch.cat([d_output, q_embed_data], dim=-1)\n",
    "        output = self.out(concat_q)\n",
    "        labels = target.reshape(-1)\n",
    "        m = nn.Sigmoid()\n",
    "        preds = output.reshape(-1)\n",
    "        mask = labels > -0.9\n",
    "        masked_lables = labels[mask].float()\n",
    "        masked_preds = preds[mask]\n",
    "        loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        output = loss(masked_preds, masked_lables)\n",
    "        return output.sum() + c_reg_loss, m(preds), mask.sum()\n",
    "\n",
    "\n",
    "class Architecture(nn.Module):\n",
    "    def __init__(self, n_blocks, d_model, d_feature, d_ff, n_heads, dropout, kq_same):\n",
    "        super(Architecture, self).__init__()\n",
    "        \"\"\"\n",
    "            n_block : number of stacked blocks in the attention\n",
    "            d_model : dimension of attention input/output\n",
    "            d_feature : dimension of input in each of the multi-head attention part.\n",
    "            n_head : number of heads. n_heads*d_feature = d_model\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.blocks_1 = nn.ModuleList([\n",
    "            TransformerLayer(d_model, d_feature, d_ff, n_heads, dropout, kq_same)\n",
    "            for _ in range(n_blocks)\n",
    "        ])\n",
    "        self.blocks_2 = nn.ModuleList([\n",
    "            TransformerLayer(d_model, d_feature, d_ff, n_heads, dropout, kq_same)\n",
    "            for _ in range(n_blocks * 2)\n",
    "        ])\n",
    "\n",
    "    def forward(self, q_embed_data, qa_embed_data):\n",
    "        x = q_embed_data\n",
    "        y = qa_embed_data\n",
    "\n",
    "        # encoder\n",
    "        for block in self.blocks_1:  # encode qas\n",
    "            y = block(mask=1, query=y, key=y, values=y)\n",
    "        flag_first = True\n",
    "        for block in self.blocks_2:\n",
    "            if flag_first:  # peek current question\n",
    "                x = block(mask=1, query=x, key=x, values=x, apply_pos=False)\n",
    "                flag_first = False\n",
    "            else:  # dont peek current response\n",
    "                x = block(mask=0, query=x, key=x, values=y, apply_pos=True)\n",
    "                flag_first = True\n",
    "        return x\n",
    "\n",
    "\n",
    "class TransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, d_ff, n_heads, dropout, kq_same):\n",
    "        super(TransformerLayer, self).__init__()\n",
    "        \"\"\"\n",
    "        This is a Basic Block of Transformer paper. It contains one Multi-head attention object. Followed by layer\n",
    "        norm and position wise feedforward net and dropout layer.\n",
    "        \"\"\"\n",
    "        kq_same = kq_same == 1\n",
    "        # Multi-Head Attention Block\n",
    "        self.masked_attn_head = MultiHeadAttention(d_model, d_feature, n_heads, dropout, kq_same=kq_same)\n",
    "\n",
    "        # Two layer norm layer and two dropout layer\n",
    "        self.layer_norm1 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "        self.layer_norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, mask, query, key, values, apply_pos=True):\n",
    "        \"\"\"\n",
    "        Input:\n",
    "            block : object of type BasicBlock(nn.Module).\n",
    "                    It contains masked_attn_head objects which is of type MultiHeadAttention(nn.Module).\n",
    "            mask : 0 means, it can peek only past values. 1 means, block can peek only current and pas values\n",
    "            query : Query. In transformer paper it is the input for both encoder and decoder\n",
    "            key : Keys. In transformer paper it is the input for both encoder and decoder\n",
    "            values: In transformer paper,\n",
    "                    it is the input for encoder and encoded output for decoder (in masked attention part)\n",
    "        Output:\n",
    "            query: Input gets changed over the layer and returned.\n",
    "        \"\"\"\n",
    "        seqlen = query.size(1)\n",
    "        nopeek_mask = np.triu(np.ones((1, 1, seqlen, seqlen)), k=mask).astype('uint8')\n",
    "        src_mask = (torch.from_numpy(nopeek_mask) == 0).to(device)\n",
    "        if mask == 0:  # If 0, zero-padding is needed.\n",
    "            # Calls block.masked_attn_head.forward() method\n",
    "            query2 = self.masked_attn_head(query, key, values, mask=src_mask, zero_pad=True)\n",
    "        else:\n",
    "            query2 = self.masked_attn_head(query, key, values, mask=src_mask, zero_pad=False)\n",
    "\n",
    "        query = query + self.dropout1(query2)\n",
    "        query = self.layer_norm1(query)\n",
    "        if apply_pos:\n",
    "            query2 = self.linear2(self.dropout(self.activation(self.linear1(query))))\n",
    "            query = query + self.dropout2(query2)\n",
    "            query = self.layer_norm2(query)\n",
    "        return query\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, d_feature, n_heads, dropout, kq_same, bias=True):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \"\"\"\n",
    "        It has projection layer for getting keys, queries and values. Followed by attention and a connected layer.\n",
    "        \"\"\"\n",
    "        self.d_model = d_model\n",
    "        self.d_k = d_feature\n",
    "        self.h = n_heads\n",
    "        self.kq_same = kq_same\n",
    "\n",
    "        self.v_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.k_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        if kq_same is False:\n",
    "            self.q_linear = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.proj_bias = bias\n",
    "        self.out_proj = nn.Linear(d_model, d_model, bias=bias)\n",
    "        self.gammas = nn.Parameter(torch.zeros(n_heads, 1, 1))\n",
    "        xavier_uniform_(self.gammas)\n",
    "\n",
    "    def forward(self, q, k, v, mask, zero_pad):\n",
    "        bs = q.size(0)\n",
    "        # perform linear operation and split into h heads\n",
    "        k = self.k_linear(k).view(bs, -1, self.h, self.d_k)\n",
    "        if self.kq_same is False:\n",
    "            q = self.q_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        else:\n",
    "            q = self.k_linear(q).view(bs, -1, self.h, self.d_k)\n",
    "        v = self.v_linear(v).view(bs, -1, self.h, self.d_k)\n",
    "\n",
    "        # transpose to get dimensions bs * h * sl * d_model\n",
    "        k = k.transpose(1, 2)\n",
    "        q = q.transpose(1, 2)\n",
    "        v = v.transpose(1, 2)\n",
    "\n",
    "        # calculate attention using function we will define next\n",
    "        scores = attention(q, k, v, self.d_k, mask, self.dropout, zero_pad, self.gammas)\n",
    "\n",
    "        # concatenate heads and put through final linear layer\n",
    "        concat = scores.transpose(1, 2).contiguous().view(bs, -1, self.d_model)\n",
    "\n",
    "        output = self.out_proj(concat)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "def attention(q, k, v, d_k, mask, dropout, zero_pad, gamma=None):\n",
    "    \"\"\"\n",
    "    This is called by Multi-head atention object to find the values.\n",
    "    \"\"\"\n",
    "    scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    bs, head, seqlen = scores.size(0), scores.size(1), scores.size(2)\n",
    "\n",
    "    x1 = torch.arange(seqlen).expand(seqlen, -1).to(device)\n",
    "    x2 = x1.transpose(0, 1).contiguous()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        scores_ = scores.masked_fill(mask == 0, -1e32)\n",
    "        scores_ = F.softmax(scores_, dim=-1)\n",
    "        scores_ = scores_ * mask.float().to(device)\n",
    "        distcum_scores = torch.cumsum(scores_, dim=-1)\n",
    "        disttotal_scores = torch.sum(scores_, dim=-1, keepdim=True)\n",
    "        position_effect = torch.abs(x1 - x2)[None, None, :, :].type(torch.FloatTensor).to(device)\n",
    "        dist_scores = torch.clamp((disttotal_scores - distcum_scores) * position_effect, min=0.)\n",
    "        dist_scores = dist_scores.sqrt().detach()\n",
    "    m = nn.Softplus()\n",
    "    gamma = -1. * m(gamma).unsqueeze(0)\n",
    "    # Now after do exp(gamma*distance) and then clamp to 1e-5 to 1e5\n",
    "    total_effect = torch.clamp(torch.clamp((dist_scores * gamma).exp(), min=1e-5), max=1e5)\n",
    "    scores = scores * total_effect\n",
    "\n",
    "    scores.masked_fill(mask == 0, -1e23)\n",
    "    scores = F.softmax(scores, dim=-1)\n",
    "    if zero_pad:\n",
    "        pad_zero = torch.zeros(bs, head, 1, seqlen).to(device)\n",
    "        scores = torch.cat([pad_zero, scores[:, :, 1:, :]], dim=2)\n",
    "    scores = dropout(scores)\n",
    "    output = torch.matmul(scores, v)\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AKT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from tqdm import trange\n",
    "\n",
    "from EduKTM import KTM\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def binary_entropy(target, pred):\n",
    "    loss = target * np.log(np.maximum(1e-10, pred)) + (1.0 - target) * np.log(np.maximum(1e-10, 1.0 - pred))\n",
    "    return np.average(loss) * -1.0\n",
    "\n",
    "\n",
    "def compute_auc(all_target, all_pred):\n",
    "    return metrics.roc_auc_score(all_target, all_pred)\n",
    "\n",
    "\n",
    "def compute_accuracy(all_target, all_pred):\n",
    "    all_pred[all_pred > 0.5] = 1.0\n",
    "    all_pred[all_pred <= 0.5] = 0.0\n",
    "    return metrics.accuracy_score(all_target, all_pred)\n",
    "\n",
    "\n",
    "def train_one_epoch(net, params, optimizer, q_data, qa_data, pid_data):\n",
    "    net.train()\n",
    "    pid_flag, batch_size, n_question, maxgradnorm = (\n",
    "        params['is_pid'], params['batch_size'], params['n_question'], params['maxgradnorm'])\n",
    "    n = int(math.ceil(len(q_data) / batch_size))\n",
    "    q_data = q_data.T\n",
    "    qa_data = qa_data.T\n",
    "    # shuffle the data\n",
    "    shuffled_ind = np.arange(q_data.shape[1])\n",
    "    np.random.shuffle(shuffled_ind)\n",
    "    q_data = q_data[:, shuffled_ind]\n",
    "    qa_data = qa_data[:, shuffled_ind]\n",
    "\n",
    "    if pid_flag:\n",
    "        pid_data = pid_data.T\n",
    "        pid_data = pid_data[:, shuffled_ind]\n",
    "\n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "\n",
    "    true_el = 0\n",
    "    for idx in trange(n):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        q_one_seq = q_data[:, idx * batch_size: (idx + 1) * batch_size]\n",
    "        qa_one_seq = qa_data[:, idx * batch_size: (idx + 1) * batch_size]\n",
    "\n",
    "        input_q = np.transpose(q_one_seq[:, :])\n",
    "        input_qa = np.transpose(qa_one_seq[:, :])\n",
    "        target = np.transpose(qa_one_seq[:, :])\n",
    "        target = (target - 1) / n_question\n",
    "        target_1 = np.floor(target)\n",
    "\n",
    "        input_q = torch.from_numpy(input_q).long().to(device)\n",
    "        input_qa = torch.from_numpy(input_qa).long().to(device)\n",
    "        target = torch.from_numpy(target_1).float().to(device)\n",
    "        if pid_flag:\n",
    "            pid_one_seq = pid_data[:, idx * batch_size: (idx + 1) * batch_size]\n",
    "            input_pid = np.transpose(pid_one_seq[:, :])\n",
    "            input_pid = torch.from_numpy(input_pid).long().to(device)\n",
    "\n",
    "            loss, pred, true_ct = net(input_q, input_qa, target, input_pid)\n",
    "        else:\n",
    "            loss, pred, true_ct = net(input_q, input_qa, target)\n",
    "        pred = pred.detach().cpu().numpy()\n",
    "        loss.backward()\n",
    "        true_el += true_ct.cpu().numpy()\n",
    "\n",
    "        if maxgradnorm > 0.:\n",
    "            torch.nn.utils.clip_grad_norm_(net.parameters(), max_norm=maxgradnorm)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # correct: 1.0; wrong 0.0; padding -1.0\n",
    "        target = target_1.reshape((-1,))\n",
    "\n",
    "        nopadding_index = np.flatnonzero(target >= -0.9)\n",
    "        nopadding_index = nopadding_index.tolist()\n",
    "        pred_nopadding = pred[nopadding_index]\n",
    "        target_nopadding = target[nopadding_index]\n",
    "\n",
    "        pred_list.append(pred_nopadding)\n",
    "        target_list.append(target_nopadding)\n",
    "\n",
    "    all_pred = np.concatenate(pred_list, axis=0)\n",
    "    all_target = np.concatenate(target_list, axis=0)\n",
    "\n",
    "    loss = binary_entropy(all_target, all_pred)\n",
    "    auc = compute_auc(all_target, all_pred)\n",
    "    accuracy = compute_accuracy(all_target, all_pred)\n",
    "\n",
    "    return loss, auc, accuracy\n",
    "\n",
    "\n",
    "def test_one_epoch(net, params, q_data, qa_data, pid_data):\n",
    "    pid_flag, batch_size, n_question = params['is_pid'], params['batch_size'], params['n_question']\n",
    "    net.eval()\n",
    "    n = int(math.ceil(len(q_data) / batch_size))\n",
    "    q_data = q_data.T\n",
    "    qa_data = qa_data.T\n",
    "    if pid_flag:\n",
    "        pid_data = pid_data.T\n",
    "    seq_num = q_data.shape[1]\n",
    "    pred_list = []\n",
    "    target_list = []\n",
    "\n",
    "    count = 0\n",
    "    true_el = 0\n",
    "\n",
    "    for idx in range(n):\n",
    "        q_one_seq = q_data[:, idx * batch_size: (idx + 1) * batch_size]\n",
    "        qa_one_seq = qa_data[:, idx * batch_size: (idx + 1) * batch_size]\n",
    "\n",
    "        input_q = np.transpose(q_one_seq[:, :])\n",
    "        input_qa = np.transpose(qa_one_seq[:, :])\n",
    "        target = np.transpose(qa_one_seq[:, :])\n",
    "        target = (target - 1) / n_question\n",
    "        target_1 = np.floor(target)\n",
    "\n",
    "        input_q = torch.from_numpy(input_q).long().to(device)\n",
    "        input_qa = torch.from_numpy(input_qa).long().to(device)\n",
    "        target = torch.from_numpy(target_1).float().to(device)\n",
    "        if pid_flag:\n",
    "            pid_one_seq = pid_data[:, idx * batch_size: (idx + 1) * batch_size]\n",
    "            input_pid = np.transpose(pid_one_seq[:, :])\n",
    "            input_pid = torch.from_numpy(input_pid).long().to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if pid_flag:\n",
    "                loss, pred, ct = net(input_q, input_qa, target, input_pid)\n",
    "            else:\n",
    "                loss, pred, ct = net(input_q, input_qa, target)\n",
    "        pred = pred.cpu().numpy()\n",
    "        true_el += ct.cpu().numpy()\n",
    "        if (idx + 1) * batch_size > seq_num:\n",
    "            real_batch_size = seq_num - idx * batch_size\n",
    "            count += real_batch_size\n",
    "        else:\n",
    "            count += batch_size\n",
    "\n",
    "        # correct: 1.0; wrong 0.0; padding -1.0\n",
    "        target = target_1.reshape((-1,))\n",
    "\n",
    "        nopadding_index = np.flatnonzero(target >= -0.9)\n",
    "        nopadding_index = nopadding_index.tolist()\n",
    "        pred_nopadding = pred[nopadding_index]\n",
    "        target_nopadding = target[nopadding_index]\n",
    "\n",
    "        pred_list.append(pred_nopadding)\n",
    "        target_list.append(target_nopadding)\n",
    "\n",
    "    assert count == seq_num, 'Seq not matching'\n",
    "\n",
    "    all_pred = np.concatenate(pred_list, axis=0)\n",
    "    all_target = np.concatenate(target_list, axis=0)\n",
    "\n",
    "    loss = binary_entropy(all_target, all_pred)\n",
    "    auc = compute_auc(all_target, all_pred)\n",
    "    accuracy = compute_accuracy(all_target, all_pred)\n",
    "\n",
    "    return loss, auc, accuracy\n",
    "\n",
    "\n",
    "class AKT(KTM):\n",
    "    def __init__(self, n_question, n_pid, n_blocks, d_model, dropout, kq_same, l2, batch_size, maxgradnorm,\n",
    "                 separate_qa=False):\n",
    "        super(AKT, self).__init__()\n",
    "        self.params = {\n",
    "            'is_pid': n_pid > 0,\n",
    "            'batch_size': batch_size,\n",
    "            'n_question': n_question,\n",
    "            'maxgradnorm': maxgradnorm,\n",
    "        }\n",
    "        self.akt_net = AKTNet(n_question=n_question, n_pid=n_pid, n_blocks=n_blocks, d_model=d_model, dropout=dropout,\n",
    "                              kq_same=kq_same, l2=l2, separate_qa=separate_qa).to(device)\n",
    "\n",
    "    def train(self, train_data, test_data=None, *, epoch: int, lr=0.002) -> ...:\n",
    "        optimizer = torch.optim.Adam(self.akt_net.parameters(), lr=lr, betas=(0.0, 0.999), eps=1e-8)\n",
    "\n",
    "        for idx in range(epoch):\n",
    "            train_loss, train_accuracy, train_acc = train_one_epoch(self.akt_net, self.params, optimizer, *train_data)\n",
    "            print(\"[Epoch %d] LogisticLoss: %.6f\" % (idx, train_loss))\n",
    "\n",
    "            if test_data is not None:\n",
    "                valid_loss, valid_accuracy, valid_acc = self.eval(test_data)\n",
    "                print(\"[Epoch %d] auc: %.6f, accuracy: %.6f\" % (idx, valid_acc, valid_accuracy))\n",
    "\n",
    "    def eval(self, test_data) -> ...:\n",
    "        self.akt_net.eval()\n",
    "        return test_one_epoch(self.akt_net, self.params, *test_data)\n",
    "\n",
    "    def save(self, filepath) -> ...:\n",
    "        torch.save(self.akt_net.state_dict(), filepath)\n",
    "        logging.info(\"save parameters to %s\" % filepath)\n",
    "\n",
    "    def load(self, filepath) -> ...:\n",
    "        self.akt_net.load_state_dict(torch.load(filepath))\n",
    "        logging.info(\"load parameters from %s\" % filepath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import DATA, PID_DATA\n",
    "\n",
    "batch_size = 64\n",
    "model_type = 'pid'\n",
    "n_question = 9 # from prepare_dataset\n",
    "n_pid = 0 # from prepare_dataset, 0 if not used\n",
    "seqlen = 200\n",
    "n_blocks = 1\n",
    "d_model = 256\n",
    "dropout = 0.05\n",
    "kq_same = 1\n",
    "l2 = 1e-5\n",
    "maxgradnorm = -1\n",
    "\n",
    "if model_type == 'pid':\n",
    "    dat = PID_DATA(n_question=n_question, seqlen=seqlen, separate_char=',')\n",
    "else:\n",
    "    dat = DATA(n_question=n_question, seqlen=seqlen, separate_char=',')\n",
    "train_data = dat.load_data('train_pid.txt')\n",
    "test_data = dat.load_data('test_pid.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger().setLevel(logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akt = AKT(n_question, n_pid, n_blocks, d_model, dropout, kq_same, l2, batch_size, maxgradnorm)\n",
    "akt.train(train_data, test_data, epoch=2)\n",
    "akt.save(\"akt.params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "akt.load(\"akt.params\")\n",
    "_, auc, accuracy = akt.eval(test_data)\n",
    "print(\"auc: %.6f, accuracy: %.6f\" % (auc, accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
