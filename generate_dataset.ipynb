{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.contrib import itertools\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "from math import prod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DQL_MODEL = {\n",
    "    \"join\": [\"inner_join\", \"outer_join\", \"self_join\"],\n",
    "    \"nesting\": [\"cte\", \"correlated_subquery\", \"uncorrelated_subquery\"],\n",
    "    \"predicates\": [\"basic_operators\", \"logical_operators\", \"set_operators\"]\n",
    "}\n",
    "\n",
    "SYNTAX_ELEMENT_COUNT_CAP = 10\n",
    "\n",
    "# SIM_PARAM_COMPLEXITY_CONVERGATION_FACTOR\n",
    "REGULATION = 0.5\n",
    "MAX_TASK_COUNT = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rgbeta(n: int, mean: float, var: float, min: float = 0, max: float = 1) -> float:\n",
    "    dmin = mean - min\n",
    "    dmax = max - mean\n",
    "\n",
    "    if dmin <= 0 or dmax <= 0:\n",
    "        raise ValueError(f\"mean must be between min = {min} and max = {max}\")\n",
    "\n",
    "    if var >= dmin * dmax:\n",
    "        raise ValueError(\n",
    "            f\"var must be less than (mean - min) * (max - mean) = {dmin * dmax}\")\n",
    "\n",
    "    mx = (mean - min) / (max - min)\n",
    "    vx = var / (max - min) ** 2\n",
    "\n",
    "    a = ((1 - mx) / vx - 1 / mx) * mx ** 2\n",
    "    b = a * (1 / mx - 1)\n",
    "\n",
    "    x = np.random.beta(a, b, n)\n",
    "    y = (max - min) * x + min\n",
    "\n",
    "    return y.tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_complexity(frequency):\n",
    "    regulation = REGULATION\n",
    "    #TODO: Stufenfunktion\n",
    "    if(frequency>=SYNTAX_ELEMENT_COUNT_CAP): return 1\n",
    "    return ((frequency)**(1/regulation))/(1+((frequency)**(1/regulation))) \n",
    "\n",
    "def calc_frequency(complexity):\n",
    "    regulation = REGULATION\n",
    "    if(complexity>=1): return SYNTAX_ELEMENT_COUNT_CAP\n",
    "    x = -complexity/(1-complexity)\n",
    "    if(x<0): x = x*-1\n",
    "    return x**regulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_random_task(dql_model: dict[str, list[str]]):\n",
    "    return {key: [random.randint(0, 7) for _ in dql_model[key]] for key in dql_model}\n",
    "\n",
    "\n",
    "def create_optimal_task(dql_model: dict[str, list[str]], learner_competency: dict[str, list[float]], scaffolding_bonus: dict[str, list[float]]):\n",
    "    return {\n",
    "        key: [\n",
    "            calc_frequency(learner_competency[key][i] + scaffolding_bonus[key][i]) for i in range(len(dql_model[key]))\n",
    "        ] for key in dql_model\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_task_complexities(task: dict[str, list[int]]):\n",
    "    return {key: calc_complexity_for_category(category) for key, category in task.items()}\n",
    "\n",
    "\n",
    "def calc_complexity_for_category(category: list[int]):\n",
    "    return list(calc_complexity(frequency) for frequency in category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_learner_scaffolded_competence_bonuses(dql_model: dict[str, list[str]], bonus_distribution: tuple[4]):\n",
    "    return {key: rgbeta(len(dql_model[key]), *bonus_distribution) for key in dql_model}\n",
    "\n",
    "\n",
    "def sample_from_snd_vectorized_and_normalize(X: list[float], mean=0.5, sd=0.1):\n",
    "    # Generate random normal samples and normalize\n",
    "    lower, upper = 0, 1\n",
    "    a, b = (lower - mean) / sd, (upper - mean) / sd\n",
    "    samples = truncnorm.rvs(a, b, loc=mean, scale=sd, size=len(X))\n",
    "    return (samples).tolist()\n",
    "\n",
    "\n",
    "def create_learner_competencies(dql_model: dict[str, list[str]], mean: float):\n",
    "    return {key: sample_from_snd_vectorized_and_normalize(dql_model[key], mean) for key in dql_model}\n",
    "\n",
    "\n",
    "def create_learner_population(learner_count: int, task_count: int, dql_model: dict[str, list[str]], mean_competency: float, bonus_distribution: tuple[4]):\n",
    "    population = {\n",
    "        \"learner_competencies\": [create_learner_competencies(dql_model, mean=mean_competency) for _ in range(learner_count)],\n",
    "        \"scaffolding_competence_bonus_per_step_and_learner\": [[create_learner_scaffolded_competence_bonuses(dql_model, bonus_distribution) for _ in range(learner_count)] for _ in range(task_count)]\n",
    "    }\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def calculate_delta(learner_competency: dict[str, list[str]], task_complexities: dict[str, list[str]], scaffolding_bonus: dict[str, list[str]]):\n",
    "    result = {}\n",
    "    for key in learner_competency:\n",
    "        result[key] = []\n",
    "        for i in range(len(learner_competency[key])):\n",
    "            k = learner_competency[key][i]\n",
    "            c = task_complexities[key][i]\n",
    "            t = scaffolding_bonus[key][i]\n",
    "            if (c <= k or c > k + t):\n",
    "                result[key].append(0)\n",
    "            else:\n",
    "                result[key].append(c - k)\n",
    "    return result\n",
    "\n",
    "\n",
    "def add_delta_to_competency(competency: dict[str, list[str]], delta: dict[str, list[str]]):\n",
    "    return {key: [competency[key][i] + delta[key][i] for i in range(len(competency[key]))] for key in competency}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_task_adaptation(learner_count: int, mean_competency: float, bonus_distribution: tuple[4]):\n",
    "\n",
    "    learner_population = create_learner_population(\n",
    "        learner_count, MAX_TASK_COUNT, DQL_MODEL, mean_competency, bonus_distribution)\n",
    "\n",
    "    simulationLog = [{\n",
    "        \"tasks\": [],\n",
    "        \"competencies\": [],\n",
    "        \"scaffolding_bonuses\": [],\n",
    "        \"deltas\": []\n",
    "    } for _ in range(learner_count)]\n",
    "\n",
    "    for i, j in itertools.product(range(MAX_TASK_COUNT), range(learner_count)):\n",
    "        learner_competency = learner_population[\"learner_competencies\"][j]\n",
    "        aggregated_competency = prod({key: prod(values) for key, values in learner_competency.items()}.values())\n",
    "        if(aggregated_competency < 1):\n",
    "            scaffolding_bonus = learner_population[\"scaffolding_competence_bonus_per_step_and_learner\"][i][j]\n",
    "\n",
    "            # task = create_random_task(dql_model)\n",
    "            task = create_optimal_task(\n",
    "                DQL_MODEL, learner_competency, scaffolding_bonus)\n",
    "\n",
    "            task_complexities = calc_task_complexities(task)\n",
    "            delta = calculate_delta(\n",
    "                learner_competency, task_complexities, scaffolding_bonus)\n",
    "\n",
    "            # update the learner competency in the global learner population\n",
    "            learner_population[\"learner_competencies\"][j] = add_delta_to_competency(\n",
    "                learner_competency, delta)    \n",
    "\n",
    "            simulationLog[j][\"tasks\"].append(task)\n",
    "            simulationLog[j][\"competencies\"].append(learner_competency)\n",
    "            simulationLog[j][\"scaffolding_bonuses\"].append(scaffolding_bonus)\n",
    "            simulationLog[j][\"deltas\"].append(delta)\n",
    "\n",
    "    return simulationLog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_simulation_log(simulationLog: dict[str, list[list[float]]], learnerId: int):\n",
    "    task_count = len(simulationLog[learnerId][\"tasks\"])\n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    for i, key in enumerate(DQL_MODEL):\n",
    "        color = colors[i % len(colors)]\n",
    "\n",
    "        aggregated_competency_values = []\n",
    "        aggregated_task_values = []\n",
    "        aggregated_competency_plus_bonus_values = []\n",
    "        for i in range(task_count):\n",
    "            # Get competency value\n",
    "            competency_values = simulationLog[learnerId][\"competencies\"][i][key]\n",
    "            competency_aggregated = sum(\n",
    "                competency_values)/len(competency_values)\n",
    "            aggregated_competency_values.append(competency_aggregated)\n",
    "\n",
    "            # Get task complexity value\n",
    "            task_values = calc_task_complexities(\n",
    "                simulationLog[learnerId][\"tasks\"][i])[key]\n",
    "            task_aggregated = sum(task_values)/len(task_values)\n",
    "            aggregated_task_values.append(task_aggregated)\n",
    "\n",
    "            scaffolding_bonus_values = simulationLog[learnerId][\"scaffolding_bonuses\"][i][key]\n",
    "            # Add the scaffolding bonus to the competency value\n",
    "            competency_plus_bonus_values = [\n",
    "                a + b for a, b in zip(scaffolding_bonus_values, competency_values)]\n",
    "            competency_plus_bonus_aggregated = sum(\n",
    "                competency_plus_bonus_values)/len(competency_plus_bonus_values)\n",
    "            aggregated_competency_plus_bonus_values.append(\n",
    "                competency_plus_bonus_aggregated)\n",
    "\n",
    "        plt.plot(range(task_count), aggregated_competency_values,\n",
    "                 color=color, label=f'{key} competency')\n",
    "        plt.plot(range(task_count), aggregated_task_values,\n",
    "                 '.', color=color, label=f'{key} task')\n",
    "        plt.plot(range(task_count), aggregated_competency_plus_bonus_values,\n",
    "                 '--', color=color, label=f'{key} competency + scaffolding bonus')\n",
    "\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(0, task_count-1)\n",
    "    plt.ylabel(\"Competency\")\n",
    "    plt.xlabel(\"Step\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\n",
    "        f\"sql_task_adaptation_{learnerId}.png\", dpi=300, bbox_inches=\"tight\")\n",
    "    \n",
    "    \n",
    "def plot_mean_simulation_log(simulationLog):\n",
    "    learner_count = len(simulationLog)\n",
    "    # Find the maximum task count among all learners\n",
    "    max_task_count = max(len(log[\"tasks\"]) for log in simulationLog)\n",
    "    \n",
    "    plt.figure(figsize=(16, 9))\n",
    "    colors = ['b', 'g', 'r', 'c', 'm', 'y', 'k']\n",
    "    \n",
    "    # For each DQL category\n",
    "    for i, key in enumerate(DQL_MODEL):\n",
    "        color = colors[i % len(colors)]\n",
    "        \n",
    "        # Initialize lists to store aggregated values\n",
    "        mean_competency_values = [[] for _ in range(max_task_count)]\n",
    "        mean_task_values = [[] for _ in range(max_task_count)]\n",
    "        mean_competency_plus_bonus_values = [[] for _ in range(max_task_count)]\n",
    "        \n",
    "        # Collect values for each learner and task\n",
    "        for learner_id in range(learner_count):\n",
    "            learner_task_count = len(simulationLog[learner_id][\"tasks\"])\n",
    "            \n",
    "            for task_id in range(learner_task_count):\n",
    "                # Get competency value\n",
    "                competency_values = simulationLog[learner_id][\"competencies\"][task_id][key]\n",
    "                competency_aggregated = sum(competency_values)/len(competency_values)\n",
    "                mean_competency_values[task_id].append(competency_aggregated)\n",
    "                \n",
    "                # Get task complexity value\n",
    "                task_values = calc_task_complexities(simulationLog[learner_id][\"tasks\"][task_id])[key]\n",
    "                task_aggregated = sum(task_values)/len(task_values)\n",
    "                mean_task_values[task_id].append(task_aggregated)\n",
    "                \n",
    "                # Get scaffolding bonus\n",
    "                scaffolding_bonus_values = simulationLog[learner_id][\"scaffolding_bonuses\"][task_id][key]\n",
    "                # Add scaffolding bonus to competency\n",
    "                competency_plus_bonus_values = [a + b for a, b in zip(scaffolding_bonus_values, competency_values)]\n",
    "                competency_plus_bonus_aggregated = sum(competency_plus_bonus_values)/len(competency_plus_bonus_values)\n",
    "                mean_competency_plus_bonus_values[task_id].append(competency_plus_bonus_aggregated)\n",
    "        \n",
    "        # Calculate means across learners for each task\n",
    "        mean_competency_across_learners = [sum(values)/len(values) if values else 0 for values in mean_competency_values]\n",
    "        mean_task_across_learners = [sum(values)/len(values) if values else 0 for values in mean_task_values]\n",
    "        mean_competency_plus_bonus_across_learners = [sum(values)/len(values) if values else 0 for values in mean_competency_plus_bonus_values]\n",
    "        \n",
    "        # Plot only valid data points (up to the maximum task count with data)\n",
    "        valid_task_count = max(i for i, values in enumerate(mean_competency_values) if values) + 1\n",
    "        \n",
    "        plt.plot(range(valid_task_count), mean_competency_across_learners[:valid_task_count],\n",
    "                color=color, label=f'{key} competency')\n",
    "        plt.plot(range(valid_task_count), mean_task_across_learners[:valid_task_count],\n",
    "                '.', color=color, label=f'{key} task')\n",
    "        plt.plot(range(valid_task_count), mean_competency_plus_bonus_across_learners[:valid_task_count],\n",
    "                '--', color=color, label=f'{key} competency + scaffolding bonus')\n",
    "    \n",
    "    plt.ylim(0, 1)\n",
    "    plt.xlim(0, valid_task_count-1)\n",
    "    plt.ylabel(\"Mean Competency\")\n",
    "    plt.xlabel(\"Task Number\")\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"sql_task_adaptation_mean.png\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def saveData(simulationLog, fileName):\n",
    "    records = []\n",
    "    order_id = 1\n",
    "    problem_id = 1\n",
    "\n",
    "    # Iterate through each student \n",
    "    for studentId in range(len(simulationLog)):\n",
    "        for taskIndex in range(len(simulationLog[studentId][\"tasks\"])):\n",
    "            task_order_id = order_id  # Create one order_id per task\n",
    "            # Iterate through categories\n",
    "            for category_i, (category_name, category_elements) in enumerate(DQL_MODEL.items()):\n",
    "                # Iterate through elements in category\n",
    "                for element_i, element in enumerate(category_elements):\n",
    "                    # Get delta for this element\n",
    "                    delta = simulationLog[studentId][\"deltas\"][taskIndex][category_name][element_i]\n",
    "                    \n",
    "                    # Create record with zero-padded IDs\n",
    "                    # Calculate position: (category_i * elements_per_category) + element_i\n",
    "                    skill_position = (category_i * len(category_elements)) + element_i + 1\n",
    "                    record = {\n",
    "                        'order_id': f'{order_id:08d}',\n",
    "                        'user_id': f'{studentId+1:06d}',\n",
    "                        'sequence_id': f'{studentId+1:06d}',\n",
    "                        'skill_id': skill_position,\n",
    "                        'problem_id': f'{problem_id:08d}',\n",
    "                        # todo: add problem Id and check if other ids are unique\n",
    "                        'correct': 1 if delta > 0 else 0\n",
    "                    }\n",
    "                    records.append(record)\n",
    "                    order_id += 1  \n",
    "            problem_id += 1\n",
    "\n",
    "    # Create and save dataframe\n",
    "    df = pd.DataFrame(records)\n",
    "    df.to_csv(fileName+'.csv', index=False, sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def createSimData(learner_count, mean_competency, bonus_distribution):\n",
    "    simulationLog = simulate_task_adaptation(learner_count, mean_competency, bonus_distribution)\n",
    "    plot_mean_simulation_log(simulationLog)\n",
    "    return simulationLog\n",
    "\n",
    "# (learner_count, mean_competency, bonus_distribution)\n",
    "# bonus_distribution = (mean, var, min, max)\n",
    "simVariations = [\n",
    "    # average start and fast learning\n",
    "    (1000, 0.5, (0.2, 0.002, 0.1, 0.3)),\n",
    "    # average start and average learning\n",
    "    (1000, 0.5, (0.1, 0.002, 0, 0.2)),\n",
    "    # average start and slow learning\n",
    "    (1000, 0.5, (0.05, 0.002, 0, 0.2)),\n",
    "    \n",
    "    # good start and fast learning\n",
    "    (1000, 0.7, (0.2, 0.002, 0.1, 0.3)),\n",
    "    # good start and average learning\n",
    "    (1000, 0.7, (0.1, 0.002, 0, 0.2)),\n",
    "    # good start and slow learning\n",
    "    (1000, 0.7, (0.05, 0.002, 0, 0.2)),\n",
    "    \n",
    "    # bad start and fast learning\n",
    "    (1000, 0.1, (0.2, 0.002, 0.1, 0.3)),\n",
    "    # bad start and average learning\n",
    "    (1000, 0.1, (0.1, 0.002, 0, 0.2)),  \n",
    "    # bad start and slow learning\n",
    "    (1000, 0.1, (0.05, 0.002, 0, 0.2)),\n",
    "]\n",
    "\n",
    "# Initialize datasets array with the correct length\n",
    "dataset = np.array([createSimData(*simVariation) for simVariation in simVariations]).flatten()\n",
    "np.random.shuffle(dataset)\n",
    "\n",
    "saveData(dataset, 'dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
